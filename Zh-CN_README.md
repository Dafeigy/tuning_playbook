# 深度学习调参指南

## 内容目录

## 这个文档的面向人群？

## 为何是一本Playbook？

## 开始新项目的指南

### 选择模型架构

### 选择优化器

### 选择批次大小

#### 确定可行的批次大小并估计训练的吞吐

#### 选择最小化训练时间的批次大小

#### 选择最小化资源开销的批次大小

#### 改变批次大小需要重新调整大部分超参数

#### 批次归一化如何与批次大小相关

### 选择初始配置

## 一种提升模型性能的科学方法

### 增量调整策略

### 【TODO】探索还是开发利用

### 选择下一轮实验的目标

### 设计下一轮的实验

#### 确定超参的属性：待研究的，待优化的和固定的

#### 建立一套研究的体系

#### 打破实验中的有效性与实用性的平衡

### 从实验结果中提炼想法

#### 确定合适的搜索边界

#### 搜索空间的取样点过少

#### 检查训练曲线

#### 使用isolation plot检测某一改变的有用性

#### 自动生成通常有用的图

### 确定采用改变训练pipeline的还是改变超参的配置

### 探索之后的总结

## 决定每轮训练的训练步数

### 当训练无计算限制时决定训练的时长

#### 使用学习率扫描选择初始最大训练步数

### 当训练有计算限制时决定训练的时长

#### 第一轮

#### 第二轮

## 额外的训练pipeline的指导

### 优化输入的pipeline

### 评估模型性能

#### 评估的设定

#### 设定周期性的评估

#### 从周期评估中选择一个样本

### 保存检查点并选择最优的检查点

### 设定实验追踪

### 批次归一化的实现细节

### 多主机pipelines的注意事项

## 常见问题

### 最棒的学习率衰减方法是什么？

### 我应该默认使用什么学习率衰减方式？

### 为什么有些论文的学习率调度方法这么复杂？

### Adam优化器的超参数该如何调整？

### 调参探索阶段时为什么使用伪随机搜索而不是更优秀的黑盒优化算法？

### 在哪可以找到伪随机搜索的实现？

### 随即搜索需要尝试多少次才能得到好的结果？

### 优化失败应该如何调试并优化？

#### 确定不稳定的工作负载

#### 常见不稳定因素的潜在修复方式

#### 学习率热身

##### 什么时候需要学习率热身

##### 如何引进公用学习率热身

#### 梯度裁剪

### 为什么称学习率和其他优化参数为超参数？他们并不是先验分布的参数啊？

### 为什么不应该直接调整批次大小来提高验证集的性能？

### 常见的优化算法的更新规则是怎样的？

#### 随机梯度下降(SGD)

#### Momentum

#### Nesterov

#### RMSProp

#### ADAM

#### NADAM

## 致谢

## 引用

## 贡献

### 贡献者许可协议

### 代码审查

### 社区指南